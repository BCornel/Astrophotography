{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6185b548-6134-427e-b260-f1af93a0661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83323/2994485359.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  , np.float : 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ORB works poorly with lots of dark pixels,\n",
    "# probably on account of uint8 quantization to zero.\n",
    "# This is a very simple brightener - you don't want to \n",
    "# use this for the final result\n",
    "def brighten(img,threshold=0.1): \n",
    "    if img.mean() > threshold:\n",
    "        return img\n",
    "    else:\n",
    "        return brighten(2*img - img**2,threshold) # Closes over [0,1]\n",
    "        \n",
    "# compute a homography (3x3 transform matrix for 2D homogenous coordinates)\n",
    "# that projects image 1 onto image 2. We use this to line up multiple images\n",
    "# of the sky that were taken at different times, with the camera at a slightly\n",
    "# different angle, etc.\n",
    "def match(im1,im2,features):\n",
    "    # OpenCV ORB requires uint8 for some reason\n",
    "    im1 = (brighten(im1)*255).astype('uint8') \n",
    "    im2 = (brighten(im2)*255).astype('uint8')\n",
    "    \n",
    "    # Identify interesting points in the image (i.e. stars)\n",
    "    det = cv2.ORB_create(nfeatures=features)\n",
    "    kp1,desc1 = det.detectAndCompute(im1,None)\n",
    "    kp2,desc2 = det.detectAndCompute(im2,None)\n",
    "    \n",
    "    # Matches up interesting points in both images, based on their descriptors\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True) \n",
    "    matches = bf.match(desc1,desc2)\n",
    "    \n",
    "    \n",
    "    # Pick the top 10% of matches (by hamming distance of their descriptor vectors)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    matches = matches[:len(matches)//10]\n",
    "    print(len(matches))\n",
    "    \n",
    "    \n",
    "    if len(matches) < 10:\n",
    "        raise Exception(\"<10 matching descriptors, poor match\")\n",
    "    # Get the coordinates of the matching stars in each image\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "    \n",
    "    # Calculate a homography matrix from our set of probably-matching stars.\n",
    "    # The RANSAC algorithm will try to discard inconsistent outliers.\n",
    "    # Mask tells us which matches seem internally consistent.\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    if mask.sum() < 10:\n",
    "       raise Exception(\"<10 RANSAC inlier descriptors, poor match\")\n",
    "    return M\n",
    "\n",
    "# Using `im` as the canvas, project every image in `loaders` onto `im` (after lining them up).\n",
    "# Returns the summed up image, as well as an array telling us what fraction of images contributed\n",
    "# to each pixel. We probably only want to use pixels that came from every image\n",
    "# We use `loaders` instead of passing in images directly because we want to \n",
    "# load huge TIFF images into memory one at a time rather than all at once.\n",
    "def add(im,loaders,features):\n",
    "    h,w,*_ = im.shape\n",
    "    out = im.copy()\n",
    "    count = np.full((h,w),1.0)\n",
    "    for load_im2 in loaders:\n",
    "        im2 = load_im2()\n",
    "        M = match(im2,im,features)\n",
    "        out += cv2.warpPerspective(im2,M,(w,h))\n",
    "        counter = np.full(im2.shape[0:2],1.0)\n",
    "        count += cv2.warpPerspective(counter,M,(w,h))\n",
    "    return (out/out.max(),count/count.max())\n",
    " \n",
    "# Normalize various formats to [0,1)       \n",
    "maxima = { np.uint8 : 255.0\n",
    "         , np.dtype('u1') : 255.0\n",
    "         , np.uint16 : 65535.0\n",
    "         , np.dtype('u2') : 65535.0\n",
    "         , np.float : 1.0\n",
    "         , np.double : 1.0\n",
    "         }\n",
    "\n",
    "# We process images mostly as 64-bit double-precision floating \n",
    "# point arrays, since they have plenty of precision.\n",
    "def load_img(path):\n",
    "    print(\"Loading %s\" % path)\n",
    "    image = imageio.imread(path)\n",
    "    maximum = maxima[image.dtype]\n",
    "    image = image.astype('double')/maximum\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fd3c1-a0b2-4991-ab4e-49d1c9c88214",
   "metadata": {},
   "source": [
    "https://blog.francium.tech/feature-detection-and-matching-with-opencv-5fd2394a590"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04177a2a-6073-4c43-a8e7-1727d07665cb",
   "metadata": {},
   "source": [
    "## Collect all images collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bb3966-c21a-4e12-8c15-c802cb231246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00866.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83323/2994485359.py:86: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00868.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00876.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00872.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00880.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00881.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00878.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00879.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00875.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00867.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00877.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00883.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00882.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00885.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00871.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00873.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00870.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00869.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00884.tif\n",
      "Loading /home/zerocool/github/astrophotography/quarter_moon/DSC00874.tif\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "photos = '/quarter_moon/' # Put in picture directory\n",
    "file_path = os.getcwd() +  photos\n",
    "images = []\n",
    "for f in os.listdir(file_path):\n",
    "    if f.endswith('tif'):\n",
    "        images.append(file_path+f)\n",
    "\n",
    "\n",
    "# Load the images and display them\n",
    "images = [load_img(path) for path in images]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80ec1f-685c-4cfb-b114-1840e6f314c9",
   "metadata": {},
   "source": [
    "## Stack images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b356e-b66a-4259-84f7-fa6c88f6a2ed",
   "metadata": {},
   "source": [
    "I have split them into 3 different orders: high, mid and low based on the feature selection count. The difference in the feature selection when stacking can be scene in the figure below. In this case more features is better stacking, but that may not always be the case and may result in over-alignment and increased artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9e69db-3d9b-4911-9f8d-a6af8b1e123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high\n",
      "920\n",
      "958\n",
      "932\n",
      "969\n",
      "953\n",
      "979\n",
      "934\n",
      "948\n",
      "937\n",
      "959\n",
      "953\n",
      "954\n",
      "947\n",
      "919\n",
      "977\n",
      "942\n",
      "965\n",
      "979\n",
      "982\n",
      "\n",
      "\n",
      "mid\n",
      "442\n",
      "438\n",
      "436\n",
      "442\n",
      "441\n",
      "432\n",
      "436\n",
      "441\n",
      "443\n",
      "436\n",
      "442\n",
      "439\n",
      "438\n",
      "441\n",
      "438\n",
      "440\n",
      "440\n",
      "443\n",
      "444\n",
      "\n",
      "\n",
      "low\n",
      "32\n",
      "34\n",
      "32\n",
      "33\n",
      "33\n",
      "31\n",
      "32\n",
      "33\n",
      "33\n",
      "32\n",
      "32\n",
      "32\n",
      "31\n",
      "31\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "main, *others = images\n",
    "print('high')\n",
    "(stacked_high, count_high) = add(main, ((lambda:img) for img in others),features=1000000)\n",
    "print('\\n')\n",
    "print('mid')\n",
    "(stacked_mid, count_mid) = add(main, ((lambda:img) for img in others),features=50000)\n",
    "print('\\n')\n",
    "print('low')\n",
    "(stacked_low, count_low) = add(main, ((lambda:img) for img in others),features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158206b-bf25-42eb-829e-d4169f2932dd",
   "metadata": {},
   "source": [
    "## Estimate the cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804f50a-95eb-4627-888f-e272600963d1",
   "metadata": {},
   "source": [
    "To estimate the cropping the original image is shown on the left and the data overlap on the right. The deep yellow is where there is 100% alignment. Everything else can be thrown out as it will decrease the SNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f591a5-14b4-4f33-95f5-9f6c227f7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title('original image', fontsize=20)\n",
    "plt.imshow(stacked_high,cmap='gray')\n",
    "plt.axis('on')\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "ax1.set_title('data overlap', fontsize=20)\n",
    "plt.imshow(count_high)\n",
    "plt.axis('on')\n",
    "plt.savefig('cropping.png', \n",
    "            dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed40f1-77b4-40a2-928e-8645dc975fcc",
   "metadata": {},
   "source": [
    "## Compare images cropped and stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34fab2-a7ad-46d3-8e4e-6ffe14d6c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    # return img[1100:2000,2500:3500] # for quarter moon\n",
    "    return img[1100:2000,2500:3500] #\n",
    "# double check that our cropped out region uses 100% of source images\n",
    "print(crop(count_high).shape)\n",
    "print(crop(count_high))\n",
    "assert((crop(count_high)==1.0).all()) # checks to make sure all values are 1 and not a fraction\n",
    "\n",
    "# cropped_original = crop(brighten(main,threshold=0.5))\n",
    "# cropped_stacked = crop(brighten(stacked,threshold=0.5))\n",
    "cropped_original = crop(main)\n",
    "img= crop(stacked_high)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title('original image', fontsize=20)\n",
    "plt.imshow(cropped_original, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "ax1.set_title('stacked image', fontsize=20)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig('original_compare.png', \n",
    "            dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226c030-1495-4dcd-9843-26d80ec2e930",
   "metadata": {},
   "source": [
    "## Comparison of stacking from high, mid and low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0c6c5-210f-4899-924d-caac6ae87b4e",
   "metadata": {},
   "source": [
    "It is clear that in this case the more features produced a better image. This is likely because the moon does not have many features to pick from that can be aligned, so in this case, the more the better. There are still artifacts produced. These artifacts are seen in the blurring and \"smearing\" around the edges of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb2309-7166-4f24-924b-6bd07f4c2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparisons\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.imshow(crop(stacked_high))\n",
    "plt.title('High feature')\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.imshow(crop(stacked_mid))\n",
    "plt.title('Mid feature')\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.imshow(crop(stacked_low))\n",
    "plt.title('Low feature')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ceee0-b5a0-42d9-a86e-3a1ec6277cc8",
   "metadata": {},
   "source": [
    "## Check noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b1013-259b-4946-8e88-5f237cae25ac",
   "metadata": {},
   "source": [
    "Though I have not yet found a good way to quantify the noise mathematically, there is a clear decrease in noise that can be seen from observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73d427-ada0-4618-8f1f-50fb0253fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    return img[1300:1900,2700:3400]\n",
    "# double check that our cropped out region uses 100% of source images\n",
    "assert((crop(count_high)==1.0).all()) \n",
    "\n",
    "cropped_original = crop(brighten(main,threshold=0.5))\n",
    "cropped_stacked = crop(brighten(stacked_high,threshold=0.5))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title('original image noise', fontsize=20)\n",
    "plt.imshow(cropped_original, cmap='gray')\n",
    "plt.axis('off')\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "ax1.set_title('stacked image noise', fontsize=20)\n",
    "plt.imshow(cropped_stacked, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig('noise_check.png', \n",
    "            dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47518b-e2f9-4016-a3b8-caa62bb75bbb",
   "metadata": {},
   "source": [
    "I cannot seem to do anything to quantify the noise. Instead I am going to take the stacked image and try and run different filters and see the results.\n",
    "https://towardsdatascience.com/massive-tutorial-on-image-processing-and-preparation-for-deep-learning-in-python-1-e534ee42f122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f0644-1a0e-46d6-80e1-df87e10b92dc",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac414a8-c050-454c-b74d-fa0a2d9abbdc",
   "metadata": {},
   "source": [
    "2d convolusional filter\n",
    "\n",
    "https://learnopencv.com/image-filtering-using-convolution-in-opencv/\n",
    "https://blog.demofox.org/2022/02/26/image-sharpening-convolution-kernels/\n",
    "\n",
    "The 2d convolution uses a kernel sharpening filter where convolution is defined as:\n",
    "\n",
    "$g(x,y = \\omega * f(x,y) = \\sum_{dx=-\\alpha}^{\\alpha} \\omega(dx,dy)f(x-dx,y-dy)$\n",
    "\n",
    "Where $g(x,y)$ is the filtered image and $f(x,y)$ is the original image kernel. The sharpening kernel used is defined as:\n",
    "\n",
    "$\\omega =  \\begin{bmatrix} 0 & -1 & 0\\\\ -1 & 5 & -1\\\\ 0 & -1 & 0 \\end{bmatrix} $\n",
    "\n",
    "The sharpening filter kernel acts as a derivative operator that sharpens the edges in an image.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b77837-1326-4774-97c8-33bfa392499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "import skimage.filters as filt k,\n",
    "\n",
    "fig,axs = plt.subplots(nrows=1, ncols=2,figsize=(10,10))\n",
    "\n",
    "# Sharpen with convolution\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                   [-1, 5,-1],\n",
    "                   [0, -1, 0]])\n",
    "image_sharp = cv2.filter2D(src=img,\n",
    "                           ddepth=-10,\n",
    "                           kernel=kernel)\n",
    "axs[0].imshow(img)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Original image',fontsize=15)\n",
    "axs[1].imshow(image_sharp)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('2d Convolution',fontsize=15)\n",
    "plt.savefig('convolution_compare.png',\n",
    "            dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876eecc9-0c86-441f-8526-457ac7ad35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=1, ncols=1,figsize=(10,10))\n",
    "axs.imshow(image_sharp)\n",
    "axs.axis('off')\n",
    "axs.set_title('2d Convolution',fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3644a-8aba-44ec-b72a-58c055aa7483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
